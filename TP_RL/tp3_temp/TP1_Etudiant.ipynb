{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOdksYCAQjQa"
      },
      "source": [
        "Pour faire tourner ce notebook, vous devez avoir les fichiers frozen_lake_deterministic_transition.npy et frozen_lake_stochastic_transition.npy au même emplacement que votre notebook. Ces deux fichiers vous permetterons de générer les environnements. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72y9j2sfQjQv"
      },
      "source": [
        "# Reinforcement Learning - Practical Session 1\n",
        "\n",
        "\n",
        "## Présentation\n",
        "\n",
        "Un processus de décision de Markov (PDM) est défini comme un tuple $(S, A, P, r, \\gamma)$ où :\n",
        "* $S$ est l'espace d'état\n",
        "* $A$ est l'espace des actions \n",
        "* $P$ représente les probabilités de transition, $P(s,a,s')$ est la probabilité d'arriver à l'état $s'$ en faisant l'action $a$ dans l'état $s$.\n",
        "* $r$ est la fonction de récompense telle que $r(s,a,s')$ est la récompense obtenue en prenant l'action $a$ dans l'état $s$ et en arrivant à $s'$.\n",
        "* $\\gamma$ est le facteur d'actualisationor\n",
        "\n",
        "Une politique déterministe $\\pi$ est une correspondance de $S$ à $A$ : $\\pi(s)$ est l'action à prendre à l'état $s$.\n",
        "\n",
        "L'objectif de l'agent est de trouver la politique $\\pi$ qui maximise la somme attendue des récompenses actualisées en suivant $\\pi$. La valeur de $\\pi$ est définie comme suit\n",
        "\n",
        "$$\n",
        "V_\\pi(s) = E\\left[ \\sum_{t=0}^\\infty \\gamma^t r(S_t, A_t, S_{t+1}) | S_0 = s \\right]\n",
        "$$\n",
        "\n",
        "On peut montrer que la fonction de valeur optimale, définie comme étant $V^*(s) = \\max_\\pi V_\\pi(s)$, satisfait l'équation de Bellman :\n",
        "\n",
        "$$\n",
        "V_\\pi(s) = \\sum_{s' \\in S}  P(s,\\pi(s),s')[r(s,\\pi(s),s') + \\gamma V_\\pi(s')]\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "V^*(s) = \\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
        "$$\n",
        "\n",
        "Il est parfois préférable de travailler avec les Q fonctions :\n",
        "\n",
        "$$\n",
        "Q_\\pi(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma  Q^\\pi(s', \\pi(s')]\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma \\max_{a'} Q^*(s', a')]\n",
        "$$\n",
        "\n",
        "tel que $V_\\pi(s) = Q_\\pi(s, \\pi(s))$ et $V^*(s) = \\max_a Q^*(s, a)$.\n",
        "\n",
        "\n",
        "### Utilisation de l'itération des valeurs pour calculer une politique optimale\n",
        "Si la fonction de récompense et les probabilités de transition sont connues (et que les espaces d'état et d'action ne sont pas très grands), nous pouvons utiliser des méthodes de programmation dynamique pour calculer $V^*(s)$. L'itération de valeurs est une façon de le faire.\n",
        "\n",
        "\n",
        "#####  Itération de valeurs pour calculer $V^*(s)$:\n",
        "$$\n",
        "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')]   \\\\\n",
        "$$\n",
        "\n",
        "\n",
        "* Pour tout $Q_0$, soit $Q_n = T^* Q_{n-1}$. \n",
        "* On a $\\lim_{n\\to\\infty}Q_n = Q^*$ et $Q^* = T^* Q^*$.\n",
        "\n",
        "\n",
        "#### Trouver la politique optimale à partir de  $V^\\pi(s)$:\n",
        "\n",
        "La politique optimale $\\pi^*$ peut être calculée de la façon suivante\n",
        "\n",
        "$$\n",
        "\\pi^*(s) \\in \\arg\\max_{a\\in A} Q^*(s, a) =  \\arg\\max_{a\\in A} \\sum_{s' \\in S}  P(s,a,s')[r(s,a,s') + \\gamma V^*(s')]\n",
        "$$\n",
        "\n",
        "###  Q-Learning et SARSA \n",
        "\n",
        "Lorsque la fonction de récompense et les probabilités de transition sont *inconnues*, nous ne pouvons pas utiliser la programmation dynamique pour trouver la fonction de valeur optimale. Q-Learning et SARSA sont des algorithmes d'approximation stochastique qui nous permettent d'estimer la fonction de valeur en utilisant uniquement des échantillons de l'environnement.\n",
        "\n",
        "#####  Q-learning\n",
        "\n",
        "L'algorithme Q-Learning nous permet d'estimer la Q fonction optimale en utilisant uniquement les trajectoires du MDP obtenues en suivant une certaine politique d'exploration. \n",
        "\n",
        "Q-learning avec une exploration de type $\\varepsilon$-greedy effectue la mise à jour suivante au temps $t$ :\n",
        "\n",
        "1. Dans l'état $s_t$, on effectue une action $a_t$ telle que $a_t$ est aléatoire avec une probabilité $\\varepsilon$ et $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ avec une probabilité $1-\\varepsilon$ ;\n",
        "2. Observer $s_{t+1}$ et la récompense $r_t$ ;\n",
        "3. Calculer $\\delta_t = r_t + \\gamma   \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$ ;\n",
        "4. Mettre à jour $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\} $.\n",
        "\n",
        "\n",
        "##### SARSA\n",
        "\n",
        "SARSA est similaire à l'apprentissage Q, mais c'est un algorithme *sur-politique* : il suit une politique (stochastique) $\\pi_Q$ et met à jour son estimation vers la valeur de cette politique. Un choix possible est le suivant :\n",
        "\n",
        "$$\n",
        "\\pi_Q(a|s) = \\frac{ \\exp(\\tau^{-1}Q(s,a))  }{\\sum_{a'}\\exp(\\tau^{-1}Q(s,a')) }\n",
        "$$\n",
        "où $\\tau$ est un paramètre de \"contrôle\" : lorsque $\\tau$ s'approche de 0, $\\pi_Q(a|s)$ se rapproche de la politique greedy (déterministe) $a \\in \\arg\\max_{a'}Q(s,a')$.\n",
        "\n",
        "À chaque instant $t$, SARSA conserve une estimation $\\hat{Q}_t$ de la vraie  Q fonction et utilise $\\pi_{\\hat{Q}_t}(a|s)$ pour choisir l'action $a_t$. Si $\\tau \\to 0$ avec un taux approprié comme $t \\to \\infty$, $\\hat{Q}_t$ converge vers $Q$ et $\\pi_{\\hat{Q}_t}(a|s)$ converge vers la politique optimale $\\pi^*$.\n",
        "\n",
        "La mise à jour SARSA au temps $t$ se fait comme suit :\n",
        "\n",
        "1. Dans l'état $s_t$, on effectue l'action $a_t \\sim \\pi_{\\hat{Q}_t}(a|s_t)$ ;\n",
        "2. Observer $s_{t+1}$ et la récompense $r_t$ ;\n",
        "3. Echantillonner l'action suivante $a_{t+1} \\sim \\pi_{\\hat{Q}_t}(a|s_{t+1})$;\n",
        "4. Calculer $\\delta_t = r_t + \\gamma \\hat{Q}_t(s_{t+1}, a_{t+1}) - \\hat{Q}_t(s_t, a_t)$.\n",
        "5. Actualisez $ \\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$.\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "Votre objectif est d'implémenter l'itération de valeurs, le Q-Learning et SARSA pour l'environnement [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/).\n",
        "\n",
        "* Dans l'exercice 1, vous implémenterez les opérateurs de Bellman $T^\\pi$ et $T^*$ et vérifierez leurs propriétés.\n",
        "* Dans l'exercice 2, vous implémenterez l'itération de valeurs.\n",
        "* Dans les exercices 3 et 4, vous implémenterez Q-Learning et SARSA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ5nZw2_cZ6H"
      },
      "source": [
        "Voici dans un premier temps quelques fonctions qui vous permetteront d'évoluer dans votre environnement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EsnPSMyKmE3L"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class FiniteEnv(ABC):\n",
        "    \"\"\"\n",
        "    Base class for a finite MDP.\n",
        "\n",
        "    Args:\n",
        "        states      (list): List of legnth S containing the indexes of the states, e.g. [0,1,2]\n",
        "        action_sets (list): List containing the actions available in each state, e.g. [[0,1], [2,3]],\n",
        "                            action_sets[i][j] returns the index of the j-th available action in state i\n",
        "        P       (np.array): Array of shape (Ns, Na, Ns) containing the transition probabilities,\n",
        "                            P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a). \n",
        "        gamma      (float): discount factor\n",
        "\n",
        "\n",
        "    Attributes:\n",
        "        Ns   (int): Number of states\n",
        "        Na   (int): Number of actions\n",
        "        actions (list): list containing all possible actions = [0, 1, ..., Na-1]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, states, action_sets, P, gamma):\n",
        "\n",
        "        self.states = states\n",
        "        self.action_sets = action_sets\n",
        "        self.actions = list(set().union(*action_sets))\n",
        "        self.Ns = len(states)\n",
        "        self.Na = len(self.actions)\n",
        "        self.P = P\n",
        "\n",
        "        self.state = 0  # initial state\n",
        "        self.gamma = gamma\n",
        "        self.reset()\n",
        "        super().__init__()\n",
        "\n",
        "    def available_actions(self, state=None):\n",
        "        \"\"\"\n",
        "        Return all actions available in a given state.\n",
        "        \"\"\"\n",
        "        if state is not None:\n",
        "            return self.action_sets[state]\n",
        "        return self.action_sets[self.state]\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to a default state.\n",
        "\n",
        "        Returns:\n",
        "            state (object)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state      (int): current state\n",
        "            action     (int): current action\n",
        "            next_state (int): next state\n",
        "\n",
        "        Returns:\n",
        "            reward (float)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute a step. Similar to gym function [1].\n",
        "        [1] https://gym.openai.com/docs/#environments\n",
        "\n",
        "        Args:\n",
        "            action (int): index of the action to take\n",
        "\n",
        "        Returns:\n",
        "            observation (object)\n",
        "            reward      (float)\n",
        "            done        (bool)\n",
        "            info        (dict)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def sample_transition(self, s, a):\n",
        "        \"\"\"\n",
        "        Sample a transition s' from P(s'|s,a).\n",
        "\n",
        "        Args:\n",
        "            s (int): index of state\n",
        "            a (int): index of action\n",
        "\n",
        "        Returns:\n",
        "            ss (int): index of next state\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d95uJUcEi02_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "class MDP(FiniteEnv):\n",
        "    \"\"\"\n",
        "    Enviroment with 3 states and 2 actions per state that gives a reward of 1 when going to the\n",
        "    last state and 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, P, bad_states=[], gamma=0.99, seed=42):\n",
        "        # Set seed\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        " \n",
        "        P = P\n",
        "        Ns, Na, _ = P.shape\n",
        "        \n",
        "        self.Ns = Ns\n",
        "        self.Na = Na\n",
        "        self.bad_states = set(bad_states)\n",
        "\n",
        "        # Initialize base class\n",
        "        states = np.arange(Ns).tolist()\n",
        "        action_sets = [np.arange(Na).tolist()]*Ns\n",
        "        super().__init__(states, action_sets, P, gamma)\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return 1.0 * (next_state == self.Ns - 1) \n",
        "\n",
        "    def reset(self, s=0):\n",
        "        self.state = s\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        \n",
        "        if self.state in self.bad_states or self.state == self.Ns-1:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "            \n",
        "        info = {}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self.P[s,a,:]\n",
        "        s_ = self.RS.choice(self.states, p = prob)\n",
        "        return s_\n",
        "    \n",
        "    def render(self):        \n",
        "        \n",
        "        env_to_print = \"S\"\n",
        "\n",
        "        \n",
        "        for state in range(1,self.Ns-1) :\n",
        "            if state % 4 == 0:\n",
        "                env_to_print += \"\\n\"\n",
        "            \n",
        "            if state in self.bad_states:\n",
        "                env_to_print += \"H\"\n",
        "            else:\n",
        "                env_to_print += \"F\"\n",
        "                    \n",
        "        env_to_print += \"G\"\n",
        "        \n",
        "        print(\"(S: starting point, safe) (F: frozen surface, safe) (H: hole, fall to your doom) (G: goal, where the frisbee is located)\")\n",
        "        print(\"=================\")\n",
        "        print(env_to_print)\n",
        "        print(\"=================\")\n",
        "        print(\"Current state\", self.state)\n",
        "        \n",
        "        \n",
        "class FrozenLake(MDP):\n",
        "    def __init__(self, gamma=0.99, deterministic=False, data_path=\"./data\"):\n",
        "        if deterministic:\n",
        "            P = np.load( \"frozen_lake_deterministic_transition.npy\")\n",
        "        else:\n",
        "            P = np.load(\"frozen_lake_stochastic_transition.npy\")\n",
        "        bad_states = [5, 7, 11, 12]\n",
        "        super().__init__(P=P, bad_states=bad_states, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wAAgA6plWoTd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ToyEnv1(FiniteEnv):\n",
        "    \"\"\"\n",
        "    Enviroment with 3 states and 2 actions per state that gives a reward of 1 when going to the\n",
        "    last state and 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.99, seed=42):\n",
        "        # Set seed\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        Ns = 3\n",
        "        Na = 2\n",
        "        P = np.zeros((Ns, Na, Ns))\n",
        "\n",
        "        P[:, 0, :] = np.array([[0.25, 0.5, 0.25], [0.1, 0.7, 0.2], [0.1, 0.8, 0.1]])\n",
        "        P[:, 1, :] = np.array([[0.3, 0.3, 0.4], [0.7, 0.2, 0.1], [0.25, 0.25, 0.5]])\n",
        "\n",
        "        # Initialize base class\n",
        "        states = np.arange(Ns).tolist()\n",
        "        action_sets = [np.arange(Na).tolist()]*Ns\n",
        "        super().__init__(states, action_sets, P, gamma)\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return 1.0 * (next_state == self.Ns - 1)\n",
        "\n",
        "    def reset(self, s=0):\n",
        "        self.state = s\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self.P[s,a,:]\n",
        "        s_ = self.RS.choice(self.states, p = prob)\n",
        "        return s_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Iz-CDZgBQjQx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from scipy.special import softmax # for SARSA\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx81eEx-QjQ7"
      },
      "source": [
        "# FrozenLake environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnP-CsIUQjQ9",
        "outputId": "5296614d-56a2-41f1-8243-a5a89f4a6825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set of states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Set of actions: [0, 1, 2, 3]\n",
            "Number of states:  16\n",
            "Number of actions:  4\n",
            "P has shape:  (16, 4, 16)\n",
            "discount factor:  0.95\n",
            "\n",
            "initial state:  0\n",
            "reward at (s=1, a=3,s'=2):  0.0\n",
            "\n",
            "random policy =  [3 0 0 1 1 1 3 3 2 2 1 1 2 3 2 2]\n",
            "(s, a, s', r):\n",
            "0 3 0 0.0\n",
            "0 3 0 0.0\n",
            "0 3 0 0.0\n",
            "0 3 0 0.0\n",
            "\n",
            "(S: starting point, safe) (F: frozen surface, safe) (H: hole, fall to your doom) (G: goal, where the frisbee is located)\n",
            "=================\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "=================\n",
            "Current state 0\n"
          ]
        }
      ],
      "source": [
        "# Creating an instance of FrozenLake\n",
        "# --- If deterministic=False, transitions are stochastic. Try both cases!\n",
        "env = FrozenLake(gamma=0.95, deterministic=True) \n",
        "# Small environment for debugging\n",
        "#env = ToyEnv1(gamma=0.95)\n",
        "\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Useful methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n",
        "\n",
        "# Visualizing the environment\n",
        "try:\n",
        "    env.render()\n",
        "except:\n",
        "    pass # render not available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbWEUdr9QjRU"
      },
      "source": [
        "# Exercise 1: Bellman operator\n",
        "\n",
        "1. Écrire une fonction qui prend en entrée un environnement et une fonction de valeur état-action $Q$ et qui renvoie l'opérateur d'optimalité de Bellman appliqué à $Q$, $T^* Q$ et la politique greedy par rapport à $Q$.\n",
        "\n",
        "\n",
        "La propriété de contraction de l'opérateur de Bellman est utilisée pour garantir la convergence d'algorithmes d'apprentissage par renforcement comme l'algorithme Q-learning et SARSA. Elle stipule que si une fonction d'utilité V est utilisée pour approximer la fonction de valeur optimale Q* pour un environnement donné, alors l'application de l'opérateur de Bellman sur V produira une fonction V' qui est plus proche de Q* que V lui-même. Cela signifie que lorsque l'algorithme itère en utilisant l'opérateur de Bellman pour mettre à jour sa fonction d'utilité, il converge vers la fonction de valeur optimale Q*. Cette propriété est importante pour garantir la convergence de l'algorithme et assurer qu'il trouve la politique optimale pour un environnement donné.\n",
        "\n",
        "2. Soit $Q_1$ et $Q_2$ des fonctions de valeur état-action. Vérifier la propriété de contraction : $\\Vert T^* Q_1 - T^* Q_2\\Vert \\leq \\gamma ||Q_1 - Q_2||$, où $||V|| = \\max_{s,a} |Q(s,a)|$.\n",
        "\n",
        "Rappelons $\n",
        "T^* Q(s,a) = \\sum_{s'}P(s'|s,a)[ r(s, a, s') + \\gamma \\max_{a'} Q(s', a')] .\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EEbRmWWUItZ7"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# My answer to 1.\n",
        "# --------------\n",
        "def bellman_operator(Q, env):\n",
        "    # Initialisation de TQ et greedy_policy\n",
        "    TQ = np.zeros((env.Ns, env.Na))\n",
        "    greedy_policy = []\n",
        "    \n",
        "    for state in env.states :\n",
        "        for action in env.actions :\n",
        "            rewards = []\n",
        "            prob = env.P[state, action, :]\n",
        "            #On parcourt tous les états suivants possibles\n",
        "            for next_state in env.states: \n",
        "                rewards.append(env.reward_func(state, action, next_state))               \n",
        "            TQ[state, action] = np.sum( prob*(rewards + env.gamma*Q.max(axis=1)))\n",
        "            \n",
        "    #On en déduit le comportement de la greedy_policy \n",
        "    greedy_policy = np.argmax(TQ, axis=1)\n",
        "    return (TQ, greedy_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eyY1PQh9Iv9L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Propriete de contraction :  True\n"
          ]
        }
      ],
      "source": [
        "# --------------\n",
        "# My answer to 2.\n",
        "# --------------\n",
        "n_simulations = 200\n",
        "\n",
        "#lancer pour n_simulation\n",
        "for i in range(n_simulations):\n",
        "    Q1 = np.random.randn(env.Ns, env.Na)\n",
        "    Q2 = np.random.randn(env.Ns, env.Na)\n",
        "\n",
        "print(\"Propriete de contraction : \",(np.abs((bellman_operator(Q1, env)[0] - bellman_operator(Q2, env)[0]).max()) / np.abs(Q1-Q2).max()) < 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSx1e54QQjR5"
      },
      "source": [
        "# Exercise 2: Value iteration\n",
        "\n",
        "1. (Fonction de valeur optimale) Ecrivez une fonction qui prend en entrée une fonction de valeur état-action initiale `Q0` et un environnement `env` et qui retourne un vecteur `Q` tel que $||T^* Q - Q ||_\\infty \\leq \\varepsilon $ et la politique greedy par rapport à $Q$.\n",
        "2. Testez la convergence de la fonction que vous avez implémentée.\n",
        "3. Affichez $Q$ et $V$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lePBq3DZJ_jm"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Your answer to 1.\n",
        "# --------------\n",
        "def value_iteration(Q0, env, epsilon):\n",
        "    #Initialisation\n",
        "    Q = Q0\n",
        "    (TQ, greedy_policy) = bellman_operator(Q, env)\n",
        "    while (np.max(np.abs(TQ - Q)) > epsilon):\n",
        "        Q = TQ\n",
        "        (TQ, greedy_policy) = bellman_operator(Q, env)  \n",
        "    return (Q, greedy_policy) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B_bRBYANKAun"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy policy :  [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
            "Erreur :  9.668818954366998e-07\n",
            "Erreur < Epsilon : True\n"
          ]
        }
      ],
      "source": [
        "# --------------\n",
        "# My answer to 2.\n",
        "# --------------\n",
        "epsilon = 1e-6\n",
        "Q0 = np.zeros((env.Ns, env.Na))\n",
        "(Q, greedy_policy) = value_iteration(Q0, env, epsilon)\n",
        "print(\"Greedy policy : \", greedy_policy)\n",
        "err = np.abs(Q - bellman_operator(Q, env)[0]).max()\n",
        "print(\"Erreur : \", err)\n",
        "print(\"Erreur < Epsilon :\", err<epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "oU6asq0jKNBp"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAd9klEQVR4nO3de3CU9dn/8U8OZBMhCYdIQiSRoLRRSECNUg4tWiIMDzJYZxi1oAitUg2FyKiAFphqMQIjQ0UKSltAC4JVOZT+gEEqII+cIYCiHINEIKEIZGPQAMn390dl+0SCiN67Vw7v18z+kd2bXNcGyHvuzWY3zDnnBABAiIVbLwAAqJ8IEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBDggYkTJyo9PV2VlZWme4waNUodO3Y03QH4rggQ8A0fffSRBgwYoGuuuUY+n0/JyckaMGCAdu/eXe3xfr9fEyZM0MiRIxUebvtfKjc3Vzt27NCSJUtM9wC+izBeCw74r3feeUf333+/mjZtql/96ldKS0vToUOH9Je//EUnT57UggUL1Ldv3yp/ZsqUKRo3bpyKi4sVHR1ttPl/3XvvvTp27JjWrl1rvQrwrQgQ8LUDBw4oMzNTqampWrt2ra6++urAbSdOnNBPf/pTffbZZ9q5c6fS0tICt7Vv316ZmZl6/fXXLda+yNtvv61+/fpp//79at26tfU6wCXxEBzwtUmTJunMmTN69dVXq8RHkhISEvTKK6/oiy++0KRJkwLXFxQUaOfOncrOzr7o882fP1+33HKLYmNjFRcXp4yMDP3xj3/8Xru9//776tevn1JTU+Xz+ZSSkqLHH39cX3755UXHXthl8eLF32sWECqcAQFfu+aaaxQVFaWCgoJLHpOWlqbz58+rsLBQkjR37lwNGDBAO3fuVEZGRuC4lStXqkePHurevbvuueceSdLHH3+s4uJivfnmm1e827Bhw3TgwAF17dpVzZo106ZNmzR79mz94he/0N///veLjm/Tpo3at2+vt95664pnAaESab0AUBOUlJTo6NGjF/1855syMzO1ZMkSlZaWKjY2Vp988okkVXlITpL++c9/Ki4uTitWrFBERMQP3m/ChAmKiYkJfPzII4/o+uuv19NPP63Dhw8rNTW1yvGtW7e+5JMmgJqCh+AASaWlpZKk2NjYbz3uwu0Xjv/8888VGRmpRo0aVTmucePGKisr08qVKz3Z7//Gp6ysTCdOnFDnzp3lnNP27dsvOr5JkyY6ceKEJ7OBYCFAgC4Oy6WUlpYqLCxMCQkJ33rcY489ph/96Efq1auXWrZsqcGDB2v58uXfe7/Dhw/roYceUtOmTdWoUSNdffXV6tatm6T/nL19k3NOYWFh33seEAo8BAdIio+PV3Jysnbu3Pmtx+3cuVMtW7ZUVFSUJKlZs2Y6f/584CG5C5o3b678/HytWLFCy5Yt07JlyzRr1iw9+OCDmjNnzhXtVlFRoTvvvFMnT57UyJEjlZ6eroYNG+rIkSN66KGHqv3l11OnTl02koA1zoCAr/Xp00cFBQVat25dtbe///77OnTokPr16xe4Lj09XZKqfeJCVFSU+vTpoz/96U86cOCAhgwZotdee0379++/or127dqlvXv36sUXX9TIkSPVt29fZWdnKzk5+ZJ/pqCgQDfccMMVzQFCjQABX3viiSd01VVXaciQIfr888+r3Hby5En95je/UVxcnIYOHRq4vlOnTpKkLVu2VDn+m38+PDxcmZmZkqTy8vIr2uvCkxj+7xNWnXOXfEp3SUmJDhw4oM6dO1/RHCDUeAgO+Nr111+v1157Tffff78yMjIueiWEU6dOaf78+VWe8da6dWu1a9dO7777rgYPHhy4/te//rVOnjypn//852rZsqU+/fRTTZ06VR06dKhyZtKqVStJ0qFDhy65V3p6uq677jo98cQTOnLkiOLi4vT222/r1KlT1R7/7rvvyjl32Wf0AeYcgCp27drlfvnLX7qkpCQXHh7uJLno6Gj30UcfVXv85MmTXaNGjdyZM2cC17311luuR48ernnz5i4qKsqlpqa6IUOGuGPHjlX5swkJCe4nP/nJZXfavXu3y87Odo0aNXIJCQnu4Ycfdjt27HCS3KxZs6oce++997quXbte+R0HQowAAZcxZ84cFxYW5h544IFqbz99+rRr2rSp+/Of/3xFn/ejjz5yktzSpUu9WNM559yxY8dcdHS0W7RokWefEwgWfgYEXMaDDz6ovLw8vf7663r66acvuj0+Pl5PPfWUJk2adEVvx/Dee++pU6dO6t27t2e7TpkyRRkZGTz8hlqBl+IBAJjgDAgAYIIAAQBMECAAgAkCBAAwUeN+EbWyslJHjx5VbGwsL6YIALWQc06lpaVKTk5WePilz3NqXICOHj2qlJQU6zUAAD9QYWGhWrZsecnba1yALryicFf9jyLVIOTzG69sEvKZF5y+s/qXVqnL2qwJ/d/xBfu6nTObbanL+ovfxjtU/rdTzOUPqmPq47/x8zqndfp/l31/rRoXoAsPu0WqgSLDQv8X16BhVMhnXmBxf61FNbK7z5H19BHe6EbnzWbzbzy0zP6Nf/3bpZf7MQpPQgAAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARNACNG3aNLVq1UrR0dHq2LGjNm3aFKxRAIBaKCgBWrBggUaMGKFx48Zp27Ztat++vXr27Knjx48HYxwAoBYKSoAmT56shx9+WIMGDdKNN96oGTNm6KqrrtJf//rXYIwDANRCngfo7Nmz2rp1q7Kzs/87JDxc2dnZWr9+/UXHl5eXy+/3V7kAAOo+zwN04sQJVVRUKDExscr1iYmJKioquuj4vLw8xcfHBy68FQMA1A/mz4IbPXq0SkpKApfCwkLrlQAAIeD52zEkJCQoIiJCxcXFVa4vLi5WUlLSRcf7fD75fD6v1wAA1HCenwFFRUXplltu0apVqwLXVVZWatWqVerUqZPX4wAAtVRQ3pBuxIgRGjhwoLKysnTbbbdpypQpKisr06BBg4IxDgBQCwUlQPfee6/+/e9/a+zYsSoqKlKHDh20fPnyi56YAACov4L2ltxDhw7V0KFDg/XpAQC1nPmz4AAA9RMBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJoL2i6g/VOOVTdSgYVTI557qcjLkMy/48ZYGJnOPl8eazJWkPVl2X+9uO780m110Ns5s9ppMs9Fm/8aTouzeZ6w+fr3PfiGt7nb54zgDAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADARab3ApZy+85QiwxqEfO6Pt4R+5gV7ss4ZTT5pNNfWmswYw+lWf9dS/08+M5s9N72lydw9svy7tmP1PeW8+25zOQMCAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwvMA5eXl6dZbb1VsbKyaN2+uu+++W3v27PF6DACglvM8QGvWrFFOTo42bNiglStX6ty5c+rRo4fKysq8HgUAqMU8fzXs5cuXV/l49uzZat68ubZu3aqf/exnXo8DANRSQX87hpKSEklS06ZNq729vLxc5eXlgY/9fn+wVwIA1ABBfRJCZWWlcnNz1aVLF7Vr167aY/Ly8hQfHx+4pKSkBHMlAEANEdQA5eTk6MMPP9T8+fMveczo0aNVUlISuBQWFgZzJQBADRG0h+CGDh2qpUuXau3atWrZ8tLvgujz+eTz+YK1BgCghvI8QM45/fa3v9XChQu1evVqpaWleT0CAFAHeB6gnJwczZs3T4sXL1ZsbKyKiookSfHx8YqJqZ/vyw4AuJjnPwOaPn26SkpKdPvtt6tFixaBy4IFC7weBQCoxYLyEBwAAJfDa8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATQX87htrmeHms4fSThrNRX3x2tpn1CoAkzoAAAEYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATERaL3ApbdY0UFSjBiGfuyfrZMhnAqG0JjPGegVAEmdAAAAjBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATAQ9QC+88ILCwsKUm5sb7FEAgFokqAHavHmzXnnlFWVmZgZzDACgFgpagL744gv1799fM2fOVJMmTYI1BgBQSwUtQDk5Oerdu7eys7O/9bjy8nL5/f4qFwBA3ReU9wOaP3++tm3bps2bN1/22Ly8PP3+978PxhoAgBrM8zOgwsJCDR8+XHPnzlV0dPRljx89erRKSkoCl8LCQq9XAgDUQJ6fAW3dulXHjx/XzTffHLiuoqJCa9eu1csvv6zy8nJFREQEbvP5fPL5fF6vAQCo4TwPUPfu3bVr164q1w0aNEjp6ekaOXJklfgAAOovzwMUGxurdu3aVbmuYcOGatas2UXXAwDqL14JAQBgIijPgvum1atXh2IMAKAW4QwIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMh+UXU72Nft3OKDAv93G47vwz90K+tyYwxm43645mD+Wazx7fuYDYbNQ9nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmIq0XqGmKzsYZTj9nOBv1xaGzCdYrAJI4AwIAGCFAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAiKAE6cuSIBgwYoGbNmikmJkYZGRnasmVLMEYBAGopz1+M9NSpU+rSpYvuuOMOLVu2TFdffbX27dunJk2aeD0KAFCLeR6gCRMmKCUlRbNmzQpcl5aW5vUYAEAt5/lDcEuWLFFWVpb69eun5s2b66abbtLMmTMveXx5ebn8fn+VCwCg7vM8QAcPHtT06dPVpk0brVixQo8++qiGDRumOXPmVHt8Xl6e4uPjA5eUlBSvVwIA1EBhzjnn5SeMiopSVlaWPvjgg8B1w4YN0+bNm7V+/fqLji8vL1d5eXngY7/fr5SUFN2uvooMa+Dlat/Jj7eEfuYFe7J4QzoEX/9PPjObPTe9pdlshM55d06rtVglJSWKi7v0m3x6fgbUokUL3XjjjVWuu+GGG3T48OFqj/f5fIqLi6tyAQDUfZ4HqEuXLtqzZ0+V6/bu3atrr73W61EAgFrM8wA9/vjj2rBhg55//nnt379f8+bN06uvvqqcnByvRwEAajHPA3Trrbdq4cKFeuONN9SuXTs999xzmjJlivr37+/1KABALeb57wFJ0l133aW77rorGJ8aAFBH8FpwAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJgIyi+ieqHL+i8V3eh8yOeuyQz5yACrVyn+7Gwzk7mStCYzxmz2MwfzzWYfOptgNptXpEZNwRkQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIlI6wUu5X87xSgyrIH1GiE1N72l9Qoh98zBfLPZ41t3MJttaebhdWazH07tajYbNQ9nQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEx4HqCKigqNGTNGaWlpiomJ0XXXXafnnntOzjmvRwEAajHPX4x0woQJmj59uubMmaO2bdtqy5YtGjRokOLj4zVs2DCvxwEAainPA/TBBx+ob9++6t27tySpVatWeuONN7Rp06Zqjy8vL1d5eXngY7/f7/VKAIAayPOH4Dp37qxVq1Zp7969kqQdO3Zo3bp16tWrV7XH5+XlKT4+PnBJSUnxeiUAQA3k+RnQqFGj5Pf7lZ6eroiICFVUVGj8+PHq379/tcePHj1aI0aMCHzs9/uJEADUA54H6M0339TcuXM1b948tW3bVvn5+crNzVVycrIGDhx40fE+n08+n8/rNQAANZznAXryySc1atQo3XfffZKkjIwMffrpp8rLy6s2QACA+snznwGdOXNG4eFVP21ERIQqKyu9HgUAqMU8PwPq06ePxo8fr9TUVLVt21bbt2/X5MmTNXjwYK9HAQBqMc8DNHXqVI0ZM0aPPfaYjh8/ruTkZA0ZMkRjx471ehQAoBbzPECxsbGaMmWKpkyZ4vWnBgDUIbwWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMOH57wEBV+LQ2QTrFeqdQ+cbWa9Qr6w4mm82u2dyB7PZ3wVnQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmIq0XQP02N72l2eyZh9eZzT50vpHZ7PGtO5jNXnE032y2lZ7JHcxmW329/aWVavKjyx/HGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATVxygtWvXqk+fPkpOTlZYWJgWLVpU5XbnnMaOHasWLVooJiZG2dnZ2rdvn1f7AgDqiCsOUFlZmdq3b69p06ZVe/vEiRP10ksvacaMGdq4caMaNmyonj176quvvvrBywIA6o4rfjXsXr16qVevXtXe5pzTlClT9Lvf/U59+/aVJL322mtKTEzUokWLdN999/2wbQEAdYanPwMqKChQUVGRsrOzA9fFx8erY8eOWr9+fbV/pry8XH6/v8oFAFD3eRqgoqIiSVJiYmKV6xMTEwO3fVNeXp7i4+MDl5SUFC9XAgDUUObPghs9erRKSkoCl8LCQuuVAAAh4GmAkpKSJEnFxcVVri8uLg7c9k0+n09xcXFVLgCAus/TAKWlpSkpKUmrVq0KXOf3+7Vx40Z16tTJy1EAgFruip8F98UXX2j//v2BjwsKCpSfn6+mTZsqNTVVubm5+sMf/qA2bdooLS1NY8aMUXJysu6++24v9wYA1HJXHKAtW7bojjvuCHw8YsQISdLAgQM1e/ZsPfXUUyorK9Mjjzyi06dPq2vXrlq+fLmio6O92xoAUOtdcYBuv/12OecueXtYWJieffZZPfvssz9oMQBA3Wb+LDgAQP1EgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgIkr/kVUwEvPHMw3m/1walez2ZZmHl5nNrtncv37mq84mm82u2dyB5O55905SQcvexxnQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMBFpvcA3OeckSed1TnLGyyDoykorzWafd+fMZlsq5WseUv56+PU+r//MvfD9/FLC3OWOCLHPPvtMKSkp1msAAH6gwsJCtWzZ8pK317gAVVZW6ujRo4qNjVVYWNgV/3m/36+UlBQVFhYqLi4uCBvWPPXxPkvc7/p0v+vjfZZq7/12zqm0tFTJyckKD7/0T3pq3ENw4eHh31rM7youLq5W/YV5oT7eZ4n7XZ/Ux/ss1c77HR8ff9ljeBICAMAEAQIAmKhzAfL5fBo3bpx8Pp/1KiFTH++zxP2uT/e7Pt5nqe7f7xr3JAQAQP1Q586AAAC1AwECAJggQAAAEwQIAGCCAAEATNSpAE2bNk2tWrVSdHS0OnbsqE2bNlmvFFR5eXm69dZbFRsbq+bNm+vuu+/Wnj17rNcKqRdeeEFhYWHKzc21XiXojhw5ogEDBqhZs2aKiYlRRkaGtmzZYr1WUFVUVGjMmDFKS0tTTEyMrrvuOj333HOXfZHL2mTt2rXq06ePkpOTFRYWpkWLFlW53TmnsWPHqkWLFoqJiVF2drb27dtns6zH6kyAFixYoBEjRmjcuHHatm2b2rdvr549e+r48ePWqwXNmjVrlJOTow0bNmjlypU6d+6cevToobKyMuvVQmLz5s165ZVXlJmZab1K0J06dUpdunRRgwYNtGzZMu3evVsvvviimjRpYr1aUE2YMEHTp0/Xyy+/rI8//lgTJkzQxIkTNXXqVOvVPFNWVqb27dtr2rRp1d4+ceJEvfTSS5oxY4Y2btyohg0bqmfPnvrqq69CvGkQuDritttuczk5OYGPKyoqXHJyssvLyzPcKrSOHz/uJLk1a9ZYrxJ0paWlrk2bNm7lypWuW7dubvjw4dYrBdXIkSNd165drdcIud69e7vBgwdXue6ee+5x/fv3N9oouCS5hQsXBj6urKx0SUlJbtKkSYHrTp8+7Xw+n3vjjTcMNvRWnTgDOnv2rLZu3ars7OzAdeHh4crOztb69esNNwutkpISSVLTpk2NNwm+nJwc9e7du8rfeV22ZMkSZWVlqV+/fmrevLluuukmzZw503qtoOvcubNWrVqlvXv3SpJ27NihdevWqVevXsabhUZBQYGKioqq/DuPj49Xx44d68T3thr3atjfx4kTJ1RRUaHExMQq1ycmJuqTTz4x2iq0KisrlZubqy5duqhdu3bW6wTV/PnztW3bNm3evNl6lZA5ePCgpk+frhEjRujpp5/W5s2bNWzYMEVFRWngwIHW6wXNqFGj5Pf7lZ6eroiICFVUVGj8+PHq37+/9WohUVRUJEnVfm+7cFttVicChP+cEXz44Ydat26d9SpBVVhYqOHDh2vlypWKjo62XidkKisrlZWVpeeff16SdNNNN+nDDz/UjBkz6nSA3nzzTc2dO1fz5s1T27ZtlZ+fr9zcXCUnJ9fp+11f1ImH4BISEhQREaHi4uIq1xcXFyspKcloq9AZOnSoli5dqvfee8+T91KqybZu3arjx4/r5ptvVmRkpCIjI7VmzRq99NJLioyMVEVFhfWKQdGiRQvdeOONVa674YYbdPjwYaONQuPJJ5/UqFGjdN999ykjI0MPPPCAHn/8ceXl5VmvFhIXvn/V1e9tdSJAUVFRuuWWW7Rq1arAdZWVlVq1apU6depkuFlwOec0dOhQLVy4UP/617+UlpZmvVLQde/eXbt27VJ+fn7gkpWVpf79+ys/P18RERHWKwZFly5dLnqK/d69e3XttdcabRQaZ86cuegdNSMiIlRZWWm0UWilpaUpKSmpyvc2v9+vjRs31o3vbdbPgvDK/Pnznc/nc7Nnz3a7d+92jzzyiGvcuLErKiqyXi1oHn30URcfH+9Wr17tjh07FricOXPGerWQqg/Pgtu0aZOLjIx048ePd/v27XNz5851V111lfvb3/5mvVpQDRw40F1zzTVu6dKlrqCgwL3zzjsuISHBPfXUU9areaa0tNRt377dbd++3UlykydPdtu3b3effvqpc865F154wTVu3NgtXrzY7dy50/Xt29elpaW5L7/80njzH67OBMg556ZOnepSU1NdVFSUu+2229yGDRusVwoqSdVeZs2aZb1aSNWHADnn3D/+8Q/Xrl075/P5XHp6unv11VetVwo6v9/vhg8f7lJTU110dLRr3bq1e+aZZ1x5ebn1ap557733qv1/PHDgQOfcf56KPWbMGJeYmOh8Pp/r3r2727Nnj+3SHuH9gAAAJurEz4AAALUPAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE/8fxfgiZT/r73UAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --------------\n",
        "# My answer to 3.\n",
        "# --------------\n",
        "#Affichage de V\n",
        "V = Q.max(axis=1)\n",
        "plt.imshow(V.reshape((4,4)))\n",
        "plt.title(\"V(s)\")\n",
        "plt.show()\n",
        "\n",
        "#Affichage de Q\n",
        "Q_show = np.zeros((12,12))\n",
        "for x in range(4):\n",
        "  for y in range(4):\n",
        "    Q_show[3*x][3*y+1] = Q[x*4+y][3] # top\n",
        "    Q_show[3*x+1][3*y] = Q[x*4+y][0] # left\n",
        "    Q_show[3*x+2][3*y+1] = Q[x*4+y][1] # bottom\n",
        "    Q_show[3*x+1][3*y+2] = Q[x*4+y][2] # right\n",
        "plt.imshow(Q_show)\n",
        "plt.title(\"Q(s, a)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viEDuDsMQjSq"
      },
      "source": [
        "# Exercise 3: Q-Learning\n",
        "\n",
        "\n",
        "L'algorithme Q-Learning nous permet d'estimer la Q fonction optimale en utilisant uniquement les trajectoires du MDP obtenues en suivant une certaine politique d'exploration. \n",
        "\n",
        "Q-learning avec une exploration de type $\\varepsilon$-greedy effectue la mise à jour suivante au temps $t$ :\n",
        "\n",
        "1. Dans l'état $s_t$, prendre une action $a_t$ telle que $a_t$ est aléatoire avec une probabilité $\\varepsilon$ et $a_t \\in \\arg\\max_a \\hat{Q}_t(s_t,a) $ avec une probabilité $1-\\varepsilon$ (**fonction d'action**) ;\n",
        "2. Observer $s_{t+1}$ et la récompense $r_t$ (**step dans l'environnement**) ;\n",
        "3. Calculer $\\delta_t = r_t + \\gamma \\max_a \\hat{Q}_t(s_{t+1}, a) - \\hat{Q}_t(s_t, a_t)$ (**à faire dans .optimize()**) ;\n",
        "4. Mettre à jour $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$ (**à faire aussi dans .optimize**)\n",
        "\n",
        "\n",
        "Implémentez l'apprentissage Q et testez sa convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ml-U2VfGLgMj"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1114529504.py, line 9)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[33], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.env =  # L'environnement de l'agent\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Implements Q-learning algorithm with epsilon-greedy exploration\n",
        "\n",
        "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, epsilon=1.0, epsilon_decay=0.9995,\n",
        "                 epsilon_min=0.25, seed=42):\n",
        "        self.env = env # L'environnement de l'agent \n",
        "        self.gamma = gamma# Taux d'actualisation\n",
        "        self.alpha = alpha # Taux d'apprentissage\n",
        "        self.learning_rate = learning_rate # Taux d'apprentissage pour Q-learning\n",
        "        self.min_learning_rate = min_learning_rate # Taux d'apprentissage minimum pour Q-learning\n",
        "        self.epsilon = epsilon # Taux d'exploration\n",
        "        self.epsilon_decay = epsilon_decay # Taux de décroissance de l'exploration\n",
        "        self.epsilon_min = epsilon_min # Taux d'exploration minimum\n",
        "        self.Q = Q # Matrice Q de l'agent\n",
        "        self.Nsa = None # Nombre de fois où l'agent est passé dans un état et a pris une action\n",
        "        self.state = state # État initial de l'agent\n",
        "        self.RS = None # Générateur de nombres aléatoires pour l'exploration\n",
        "\n",
        "    def get_delta(self, r, x, a, y, done):\n",
        "        \"\"\"\n",
        "        Calcule la différence entre la valeur Q actuelle et la valeur Q attendue pour l'état et l'action courants\n",
        "        :param r: reward (récompense obtenue)\n",
        "        :param x: current state (état courant)\n",
        "        :param a: current action (action courante)\n",
        "        :param y: next state (prochain état)\n",
        "        :param done: indique si la partie est finie ou non\n",
        "        :return: la différence\n",
        "        \"\"\"\n",
        "        max_q_y_a = # La valeur Q maximale pour le prochain état\n",
        "        q_x_a =  # La valeur Q actuelle pour l'état et l'action courants\n",
        "\n",
        "        return r + self.gamma*max_q_y_a - q_x_a\n",
        "\n",
        "    def get_learning_rate(self, s, a):\n",
        "      \"\"\"\n",
        "      Utilisé pour calculer le taux d'apprentissage pour l'agent en fonction de l'état courant (s) et de l'action courante (a).\n",
        "      \"\"\"\n",
        "        if self.learning_rate is None:\n",
        "            return max(1.0/max(1.0, self.Nsa[s, a])**self.alpha, self.min_learning_rate)\n",
        "        else:\n",
        "            return max(self.learning_rate, self.min_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-jKv0obLiJG"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Convergence of Q-Learning\n",
        "# ---------------------------\n",
        "\n",
        "# Number of Q learning iterations\n",
        "n_steps = int(5e5)  \n",
        "#n_steps = 10\n",
        "\n",
        "# Get optimal value function and its greedy policy\n",
        "Q0 =  # Initialisation de la matrice Q \n",
        "Q_opt, pi_opt = # Obtention de la valeur optimale de Q et de la politique optimale\n",
        "\n",
        "# Create qlearning object\n",
        "qlearning =  # création de l'objet QLearning en utilisant l'environnement et le taux d'actualisation\n",
        "\n",
        "# Iterate\n",
        "tt = 0\n",
        "Q_est =  # matrice pour stocker les estimation de Q\n",
        "while tt < n_steps:\n",
        "    qlearning.step() # Mettre à jour la matrice Q\n",
        "    # Store estimate of Q*\n",
        "    Q_est[tt, :, :] = qlearning.Q # stocker l'estimation de Q à chaque itération\n",
        "    tt +=1\n",
        "\n",
        "# Compute greedy policy (with estimated Q)\n",
        "greedy_policy = np.argmax(qlearning.Q, axis=1)\n",
        "\n",
        "# Plot\n",
        "diff = np.abs(Q_est - Q_opt).mean(axis=(1,2))\n",
        "plt.plot(diff)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning convergence\")\n",
        "\n",
        "print(env.render())\n",
        "print(\"optimal policy: \", pi_opt)\n",
        "print(\"est policy:\", greedy_policy)\n",
        "\n",
        "for state in env.states:\n",
        "    print(state)\n",
        "    print(\"true: \", Q_opt[state, :])\n",
        "    print(\"est: \", Q_est[-1,state, :])\n",
        "    print(\"----------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkD9ZMc_QjTH"
      },
      "source": [
        "# Exercise 4: SARSA\n",
        "\n",
        "SARSA est similaire au Q learning, mais c'est un algorithme *on policy* : il suit une politique (stochastique) $\\pi_Q$ et met à jour son estimation vers la valeur de cette politique. Un choix possible est :\n",
        "\n",
        "$$\n",
        "\\pi_Q(a|s) = \\frac{ \\exp(\\tau^{-1}Q(s,a))  }{\\sum_{a'}\\exp(\\tau^{-1}Q(s,a')) }\n",
        "$$\n",
        "où $\\tau$ est un paramètre de \"controle\" : lorsque $\\tau$ s'approche de 0, $\\pi_Q(a|s)$ se rapproche de la politique greedy (déterministe) $a \\in \\arg\\max_{a'}Q(s,a')$.\n",
        "\n",
        "À chaque instant $t$, SARSA conserve une estimation $\\hat{Q}_t$ de la vraie Q fonction  et utilise $\\pi_{\\hat{Q}_t}(a|s)$ pour choisir l'action $a_t$. Si $\\tau \\to 0$ avec un taux approprié comme $t \\to \\infty$, $\\hat{Q}_t$ converge vers $Q$ et $\\pi_{\\hat{Q}_t}(a|s)$ converge vers la politique optimale $\\pi^*$. \n",
        "\n",
        "La mise à jour SARSA au temps $t$ est effectuée comme suit :\n",
        "\n",
        "1. Dans l'état $s_t$, on effectue l'action $a_t \\sim \\pi_{\\hat{Q}_t}(a|s_t)$ ;\n",
        "2. Observer $s_{t+1}$ et la récompense $r_t$ ;\n",
        "3. Echantillonner l'action suivante $a_{t+1} \\sim \\pi_{\\hat{Q}_t}(a|s_{t+1})$;\n",
        "\n",
        "4. Calculer $\\delta_t = r_t + \\gamma \\hat{Q}_t(s_{t+1}, a_{t+1}) - \\hat{Q}_t(s_t, a_t)$;\n",
        "5. Actualisez $\\hat{Q}_{t+1}(s, a) = \\hat{Q}_t(s, a) + \\alpha_t(s,a)\\delta_t\\mathbb{1}\\{s=s_t, a=a_t\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ySBG-wNHf1"
      },
      "outputs": [],
      "source": [
        "#-------------------------------\n",
        "# SARSA implementation\n",
        "# ------------------------------\n",
        "\n",
        "class Sarsa:\n",
        "    \"\"\"\n",
        "    Implements SARSA algorithm.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate=None, tau=1.0): # Again, those are suggestions, you can add more arguments\n",
        "        pass\n",
        "    def act():\n",
        "        pass\n",
        "    def optimize():\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SijRIA58NIvC"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Convergence of SARSA\n",
        "# ---------------------------\n",
        "\n",
        "# Create SARSA object\n",
        "sarsa = Sarsa(env, gamma=env.gamma)\n",
        "\n",
        "# Again, you can use Q_opt and pi_opt from value_iteration to check sarsa's convergence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PoRzvSbNPmI"
      },
      "source": [
        "Comment ces deux algorithmes se comportent-ils ? \n",
        "Trouvent-ils tous les deux la politique optimale ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6SZTfCUQjTf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TP_Apprentissage",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "1e8b2337a5f82f3761e6913831a1736232f5dcfd3c7c12c6566b041b458132d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
